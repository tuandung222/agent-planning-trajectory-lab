{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference and Trajectory Training Lab\n",
        "\n",
        "This notebook is a hands-on lab for Data Scientists and AI Researchers.\n",
        "\n",
        "What you will do:\n",
        "- Run end-to-end inference with the LangGraph workflow.\n",
        "- Generate trajectory logs (`jsonl` + `summary`).\n",
        "- Validate trajectory schema quality for training.\n",
        "- Extract training-friendly samples from tool interactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow\n",
        "\n",
        "1. Environment and project checks\n",
        "2. Inference run (`main_langgraph.py`)\n",
        "3. Trace inspection\n",
        "4. Trajectory-to-training extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / 'main_langgraph.py').exists() and (candidate / 'tools.py').exists():\n",
        "            return candidate\n",
        "    raise RuntimeError('Cannot locate project root containing main_langgraph.py and tools.py')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd().resolve())\n",
        "os.chdir(PROJECT_ROOT)\n",
        "load_dotenv(PROJECT_ROOT / '.env')\n",
        "\n",
        "print('PROJECT_ROOT =', PROJECT_ROOT)\n",
        "print('PYTHON =', sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configuration\n",
        "TOPIC = 'AI agent planning strategy market snapshot 2026'\n",
        "MODEL = os.getenv('OPENAI_MODEL', 'gpt-4.1-mini')\n",
        "PROVIDER = 'openai'\n",
        "RUN_INFERENCE = True  # Set False to reuse the latest existing trace\n",
        "\n",
        "# Output directories for this notebook run\n",
        "NOTEBOOK_RUN_DIR = PROJECT_ROOT / 'test_outputs' / 'notebook_lab'\n",
        "TRACE_DIR = NOTEBOOK_RUN_DIR / 'trajectories'\n",
        "REPORT_PATH = NOTEBOOK_RUN_DIR / 'inference_report.md'\n",
        "NOTEBOOK_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TRACE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# API key diagnostics (masked)\n",
        "api_key = os.getenv('OPENAI_API_KEY', '')\n",
        "print('OPENAI_API_KEY set =', bool(api_key))\n",
        "if api_key:\n",
        "    print('OPENAI_API_KEY prefix =', api_key[:7] + '...')\n",
        "else:\n",
        "    raise RuntimeError('Missing OPENAI_API_KEY in environment.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Run end-to-end inference via CLI\n",
        "if RUN_INFERENCE:\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        'main_langgraph.py',\n",
        "        TOPIC,\n",
        "        '--provider', PROVIDER,\n",
        "        '--model', MODEL,\n",
        "        '--output', str(REPORT_PATH),\n",
        "        '--trace-dir', str(TRACE_DIR),\n",
        "    ]\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    # Force no-key search mode: DuckDuckGo + Wikipedia\n",
        "    env['SERPER_API_KEY'] = ''\n",
        "\n",
        "    print('Running command:')\n",
        "    print(' '.join(cmd))\n",
        "    completed = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=str(PROJECT_ROOT),\n",
        "        env=env,\n",
        "        text=True,\n",
        "        capture_output=True,\n",
        "        check=True,\n",
        "    )\n",
        "\n",
        "    print('Return code:', completed.returncode)\n",
        "    print('STDOUT preview:')\n",
        "    print(completed.stdout[:1200])\n",
        "    if completed.stderr:\n",
        "        print('STDERR preview:')\n",
        "        print(completed.stderr[:1200])\n",
        "else:\n",
        "    print('RUN_INFERENCE=False, skipping inference step.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Locate the latest trajectory files\n",
        "summary_files = sorted(TRACE_DIR.glob('run_*.summary.json'), key=lambda p: p.stat().st_mtime)\n",
        "jsonl_files = sorted(TRACE_DIR.glob('run_*.jsonl'), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "if not summary_files or not jsonl_files:\n",
        "    raise RuntimeError('No trajectory files found. Run inference first.')\n",
        "\n",
        "SUMMARY_PATH = summary_files[-1]\n",
        "JSONL_PATH = jsonl_files[-1]\n",
        "\n",
        "summary = json.loads(SUMMARY_PATH.read_text())\n",
        "print('SUMMARY_PATH =', SUMMARY_PATH)\n",
        "print('JSONL_PATH =', JSONL_PATH)\n",
        "print('status =', summary.get('status'))\n",
        "print('event_count =', summary.get('event_count'))\n",
        "print('tool_call_count =', summary.get('tool_call_count'))\n",
        "print('tool_error_count =', summary.get('tool_error_count'))\n",
        "print('report_len =', summary.get('report_len'))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Validate trajectory schema and event coverage\n",
        "rows = [json.loads(line) for line in JSONL_PATH.read_text().splitlines() if line.strip()]\n",
        "required_top = {'ts_utc', 'run_id', 'idx', 'event_type', 'payload'}\n",
        "missing_top_rows = [i for i, r in enumerate(rows, 1) if not required_top.issubset(r.keys())]\n",
        "\n",
        "event_counts = Counter(r['event_type'] for r in rows)\n",
        "print('rows =', len(rows))\n",
        "print('missing_top_rows =', len(missing_top_rows))\n",
        "print('event_counts =', dict(event_counts))\n",
        "\n",
        "required_events = {\n",
        "    'run_started',\n",
        "    'phase',\n",
        "    'tool_call',\n",
        "    'tool_result',\n",
        "    'message_snapshot',\n",
        "    'run_completed',\n",
        "    'final_report',\n",
        "}\n",
        "missing_events = sorted(required_events - set(event_counts.keys()))\n",
        "print('missing_required_events =', missing_events)\n",
        "if missing_top_rows or missing_events:\n",
        "    raise AssertionError('Trajectory schema/event validation failed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build training-friendly samples from tool interactions\n",
        "# Sample schema: input state (tool call) + outcome (tool result)\n",
        "\n",
        "tool_calls = [r for r in rows if r['event_type'] == 'tool_call']\n",
        "tool_results = [r for r in rows if r['event_type'] == 'tool_result']\n",
        "\n",
        "paired_count = min(len(tool_calls), len(tool_results))\n",
        "training_samples = []\n",
        "for i in range(paired_count):\n",
        "    call = tool_calls[i]\n",
        "    result = tool_results[i]\n",
        "    training_samples.append({\n",
        "        'run_id': call['run_id'],\n",
        "        'step_index': i + 1,\n",
        "        'tool': call['payload'].get('tool'),\n",
        "        'tool_kwargs': call['payload'].get('kwargs'),\n",
        "        'ok': result['payload'].get('ok'),\n",
        "        'latency_ms': result['payload'].get('latency_ms'),\n",
        "        'result_preview': result['payload'].get('result_preview'),\n",
        "    })\n",
        "\n",
        "print('tool_calls =', len(tool_calls))\n",
        "print('tool_results =', len(tool_results))\n",
        "print('paired_training_samples =', len(training_samples))\n",
        "print('first_sample =', training_samples[0] if training_samples else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Persist extracted dataset for downstream training pipelines\n",
        "DATASET_DIR = NOTEBOOK_RUN_DIR / 'datasets'\n",
        "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dataset_jsonl = DATASET_DIR / f\"{SUMMARY_PATH.stem}.training_samples.jsonl\"\n",
        "with dataset_jsonl.open('w', encoding='utf-8') as f:\n",
        "    for sample in training_samples:\n",
        "        f.write(json.dumps(sample, ensure_ascii=True) + '\\n')\n",
        "\n",
        "print('Saved training samples:', dataset_jsonl)\n",
        "print('Sample count:', len(training_samples))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: inspect the generated report\n",
        "if REPORT_PATH.exists():\n",
        "    report_text = REPORT_PATH.read_text(encoding='utf-8')\n",
        "    print('Report size:', len(report_text))\n",
        "    print('Report preview:')\n",
        "    print(report_text[:1200])\n",
        "else:\n",
        "    print('Report not found at', REPORT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Add reward labels (quality, factuality, citation quality) to each tool step.\n",
        "- Merge multiple runs into one curated training corpus.\n",
        "- Build a train/validation split by topic and source diversity.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}