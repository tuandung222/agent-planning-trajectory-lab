{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Pattern Comparison: ReAct vs Plan-and-Execute\n",
        "\n",
        "This notebook runs a simple simulation to compare orchestration cost.\n"
      ],
      "id": "3accc057"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import statistics\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    for candidate in [start, *start.parents]:\n",
        "        if (candidate / 'README.md').exists() and (candidate / 'main_langgraph.py').exists():\n",
        "            return candidate\n",
        "    return start\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd().resolve())\n",
        "os.chdir(PROJECT_ROOT)\n",
        "print('PROJECT_ROOT =', PROJECT_ROOT)\n"
      ],
      "id": "dcb1981c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "\n",
        "def simulate_react(complexity: int, uncertainty: int) -> dict:\n",
        "    loops = complexity + max(1, uncertainty // 2)\n",
        "    llm_calls = loops * 2\n",
        "    tool_calls = loops\n",
        "    return {'llm_calls': llm_calls, 'tool_calls': tool_calls, 'loops': loops}\n",
        "\n",
        "\n",
        "def simulate_plan_execute(complexity: int, uncertainty: int) -> dict:\n",
        "    plan_steps = max(3, complexity)\n",
        "    replans = 1 if uncertainty >= 8 else 0\n",
        "    llm_calls = 2 + replans\n",
        "    tool_calls = plan_steps\n",
        "    return {'llm_calls': llm_calls, 'tool_calls': tool_calls, 'loops': 1 + replans}\n"
      ],
      "id": "9e8ebdf1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tasks = [\n",
        "    {'name': 'deterministic research', 'complexity': 7, 'uncertainty': 3},\n",
        "    {'name': 'semi-dynamic analysis', 'complexity': 8, 'uncertainty': 6},\n",
        "    {'name': 'chaotic environment', 'complexity': 6, 'uncertainty': 9},\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for t in tasks:\n",
        "    react = simulate_react(t['complexity'], t['uncertainty'])\n",
        "    plan = simulate_plan_execute(t['complexity'], t['uncertainty'])\n",
        "    rows.append((t['name'], react['llm_calls'], plan['llm_calls'], react['tool_calls'], plan['tool_calls']))\n",
        "\n",
        "for row in rows:\n",
        "    print(row)\n"
      ],
      "id": "78c24459"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "react_llm_avg = statistics.mean([r[1] for r in rows])\n",
        "plan_llm_avg = statistics.mean([r[2] for r in rows])\n",
        "print('avg_react_llm_calls =', react_llm_avg)\n",
        "print('avg_plan_llm_calls =', plan_llm_avg)\n",
        "\n",
        "assert plan_llm_avg < react_llm_avg\n",
        "print('Comparison check passed: planning uses fewer LLM calls in this setup.')\n"
      ],
      "id": "acb8a0b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "\n",
        "- ReAct is more adaptive.\n",
        "- Plan-and-Execute often reduces LLM call count and control variance.\n"
      ],
      "id": "8c332e0d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}